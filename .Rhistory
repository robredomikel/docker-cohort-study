data_m.optimal_1
m.optimal_1 <- matchit(as.factor(df$MS.NonMS) ~ as.factor(main_language), data = df, method = "optimal", ratio = 1)
summary(m.optimal_1) # 1:1 tolerance
data_m.optimal_1 <- match.data(m.optimal_1)
logistic_model <- glm(factor(raw_df$velocity_bool_end) ~ velocity_mean_start+factor(MS.NonMS)+size+main_language+
n_languages+factor(creation_year)+n_commits+n_issues+n_contributors, family=binomial(link='logit'), data = data_m.optimal_1)
non_discarded_data
logistic_model <- glm(factor(non_discarded_data$velocity_bool_end) ~ velocity_mean_start+factor(MS.NonMS)+size+main_language+
n_languages+factor(creation_year)+n_commits+n_issues+n_contributors, family=binomial(link='logit'), data = data_m.exact_1)
summary(logistic_model)
logistic_model <- glm(factor(non_discarded_data$velocity_bool_end) ~ velocity_mean_start+MS.NonMS+size+main_language+
n_languages+creation_year+n_commits+n_issues+n_contributors, family=binomial(link='logit'), data = data_m.exact_1)
summary(logistic_model)
data_m.optimal_1
# 1. Get row names of the matched data
matched_rows_optimal <- rownames(data_m.optimal_1)
# 2. Get row names of the original data
original_rows_optimal <- rownames(df)
# 3. Find rows that were discarded (those in original not in matched)
discarded_rows_optimal <- setdiff(original_rows, matched_rows)
# 4. Extract discarded rows from the original dataframe
discarded_data_optimal <- raw_df[discarded_rows, ]
non_discarded_data_optimal <- raw_df[matched_rows, ]
logistic_model <- glm(factor(non_discarded_data_optimal$velocity_bool_end) ~ velocity_mean_start+MS.NonMS+size+main_language+
n_languages+creation_year+n_commits+n_issues+n_contributors, family=binomial(link='logit'), data = data_m.optimal_1)
length(non_discarded_data_optimal)
matched_rows
m.optimal_1 <- matchit(as.factor(df$MS.NonMS) ~ as.factor(main_language), data = df, method = "optimal", ratio = 1)
summary(m.optimal_1) # 1:1 tolerance
data_m.optimal_1 <- match.data(m.optimal_1)
data_long_optimal_1 <- pivot_longer(data_m.optimal_1, cols = c(velocity_mean_start, size:n_contributors), names_to = "Covariate", values_to = "Value")
data_long_optimal_1
# 1. Get row names of the matched data
matched_rows_optimal <- rownames(data_m.optimal_1)
# 2. Get row names of the original data
original_rows_optimal <- rownames(df)
# 3. Find rows that were discarded (those in original not in matched)
discarded_rows_optimal <- setdiff(original_rows_optimal, matched_rows_optimal)
# 4. Extract discarded rows from the original dataframe
discarded_data_optimal <- raw_df[discarded_rows_optimal, ]
non_discarded_data_optimal <- raw_df[matched_rows_optimal, ]
logistic_model <- glm(factor(non_discarded_data_optimal$velocity_bool_end) ~ velocity_mean_start+MS.NonMS+size+main_language+
n_languages+creation_year+n_commits+n_issues+n_contributors, family=binomial(link='logit'), data = data_m.optimal_1)
summary(logistic_model)
logistic_model <- glm(factor(non_discarded_data_optimal$velocity_bool_end) ~ velocity_mean_start+factor(MS.NonMS)+size+factor(main_language)+
n_languages+factor(creation_year)+n_commits+n_issues+n_contributors, family=binomial(link='logit'), data = data_m.optimal_1)
summary(logistic_model)
best_model <- step(logistic_model)
# Conditional plot of the power of the independent variable
coplot(velocity_mean_end~velocity_mean_start|MS.NonMS, col = "red", data = df)
# Conditional plot of the power of the independent variable
coplot(velocity_mean_end~velocity_mean_start|MS.NonMS, col = "red", rows = 1, data = df)
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
hist(controls_endvelocity, col = 'red', main='CONTROLS')
# Through histograms
hist(log(cases_endvelocity), col='steelblue', main='CASES')
hist(log(controls_endvelocity), col = 'red', main='CONTROLS')
hist(controls_endvelocity, col = 'red', main='CONTROLS')
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
# Through interquantile plots
par(mfrow=c(1,2))
qqnorm(cases_endvelocity, main='CASES')
qqline(cases_endvelocity)
qqnorm(controls_endvelocity, main='CONTROLS')
qqline(controls_endvelocity)
par(mfrow=c(1,1))
# Through interquantile plots
par(mfrow=c(1,2))
qqnorm(log(cases_endvelocity), main='CASES')
qqline(log(cases_endvelocity))
qqnorm(log(controls_endvelocity), main='CONTROLS')
# Through interquantile plots
par(mfrow=c(1,2))
qqnorm(cases_endvelocity, main='CASES')
qqline(cases_endvelocity)
qqnorm(controls_endvelocity, main='CONTROLS')
qqline(controls_endvelocity)
par(mfrow=c(1,1))
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
hist(controls_endvelocity, col = 'red', main='CONTROLS')
# Through interquantile plots
par(mfrow=c(1,2))
qqnorm(cases_endvelocity, main='CASES')
qqline(cases_endvelocity)
qqnorm(controls_endvelocity, main='CONTROLS')
qqline(controls_endvelocity)
# Through Shapiro-Wilk test:
shapiro.test(cases_endvelocity) # For cases
# Through Shapiro-Wilk test:
shapiro.test(cases_endvelocity) # For cases
help("shapiro.test")
mean(cases_endvelocity)
mean (controls_endvelocity)
sd(cases_endvelocity)
sd(controls_endvelocity)
(sd(controls_endvelocity))^2
(sd(cases_endvelocity))^2
help(fisher.test)
sample(cases_endvelocity, size = 30)
cases_endvelocity
hist(sample(cases_endvelocity, size = 30))
hist(sample(controls_endvelocity, size = 30))
hist(sample(controls_endvelocity, size = 30), freq = T)
hist(sample(controls_endvelocity, size = 30), freq = F)
hist(sample(controls_endvelocity, size = 30), density = T)
# 3. Sample means are normally distributed
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,2))
cases_sample_mean <- mean(sample(cases_endvelocity, size = 30))
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = cases_sample_mean, sd=sd(cases_endvelocity)), add = T, col="red")
controls_sample_mean <- mean(sample(controls_endvelocity, size = 30))
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_endvelocity)), add = T, col="red")
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
# 3. Sample means are normally distributed
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,2))
# 3. Sample means are normally distributed
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,2))
cases_sample_mean <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = cases_sample_mean, sd=sd(cases_endvelocity)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_endvelocity)), add = T, col="red")
# 3. Sample means are normally distributed
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,2))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_endvelocity)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_endvelocity)), add = T, col="red")
# 3. Sample means are normally distributed
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,2))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_sample)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_sample)), add = T, col="red")
# 3. Sample means are normally distributed
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,1))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_sample)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_sample)), add = T, col="red")
# The variances between the groups are not equal
(sd(cases_endvelocity))^2
(sd(controls_endvelocity))^2
hep(wilcox.test())
help(wilcox.test())
help(wilcox.test
)
## Wilcoxon rank sum test (Non parametric) ~ two sample, hence MANN-WHITNEY
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "greater")
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "two.sided")
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "less")
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "less")
## Wilcoxon rank sum test (Non parametric) ~ two sample, hence MANN-WHITNEY
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "two.sided")
# Observational boxplots of the groups.
data_long <- pivot_longer(df, cols = c(velocity_mean_start, size, creation_year:trimmed_languages), names_to = "Covariate", values_to = "Value")
library(MatchIt)
library(optmatch)
library(Matching)
library(ggplot2)
library(tidyverse)
library(patchwork)
library(plotly)
# Function to generate Q-Q plots for two groups
qq_plot <- function(data, covariate_name, group_column) {
# Filter data for the specific covariate
subset_data <- data[data$Covariate == covariate_name, ]
# Create a QQ plot comparing groups
ggplot(subset_data, aes(sample = Value, colour = !!sym(group_column))) +
stat_qq() +
stat_qq_line() +
labs(title = paste("Q-Q Plot for", covariate_name),
subtitle = "Comparing Quantiles between Groups",
x = "Treatment cases",
y = "Controls") +
scale_fill_continuous(type = "gradient") +
theme_minimal()
}
# Observational boxplots of the groups.
data_long <- pivot_longer(df, cols = c(velocity_mean_start, size, creation_year:trimmed_languages), names_to = "Covariate", values_to = "Value")
ggplot(data_long, aes(x = Covariate, y = Value, fill = MS.NonMS)) +
geom_boxplot() + # Draw boxplots
theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate x-axis labels for clarity
labs(title = "Comparison of Covariates between Treatment and Control Groups",
x = "Covariate",
y = "Value") +
facet_wrap(~ Covariate, scales = "free") # Create a separate plot for each covariate
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
# Same based on creation year
df_creation <- df %>%
group_by(creation_year, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
ggplot(df_creation, aes(x = creation_year, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Creation Year",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
m.exact_1 <- matchit(as.factor(df$MS.NonMS) ~ as.factor(main_language), data = df, method = "exact")
summary(m.exact_1)
data_m.exact_1 <- as.data.frame(match.data(m.exact_1))
data_long_exact_1 <- pivot_longer(data_m.exact_1, cols = c(velocity_mean_start, size:n_contributors), names_to = "Covariate", values_to = "Value")
# Q-Q plot
plots <- lapply(unique(data_long_exact_1$Covariate), function(cov) {
qq_plot(data_long_exact_1, cov, "MS.NonMS")
})
wrap_plots(plots, ncol = 2) # Q-Q plots for matching through different variables (We consider all variables, do not pivot only with one)
# Boxplots
ggplot(data_long_exact_1, aes(x = Covariate, y = Value, fill = MS.NonMS)) +
geom_boxplot() + # Draw boxplots
theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate x-axis labels for clarity
labs(title = "Comparison of Covariates between Treatment and Control Groups (Exact Matching)",
x = "Covariate",
y = "Value") +
facet_wrap(~ Covariate, scales = "free") # Create a separate plot for each covariate
# 1. Get row names of the matched data
matched_rows <- rownames(data_m.exact_1)
# 2. Get row names of the original data
original_rows <- rownames(df)
# 3. Find rows that were discarded (those in original not in matched)
discarded_rows <- setdiff(original_rows, matched_rows)
# 4. Extract discarded rows from the original dataframe
discarded_data <- raw_df[discarded_rows, ]
non_discarded_data <- raw_df[matched_rows, ]
# Check the first few rows of the discarded data
head(discarded_data)
model <- lm(velocity_mean_end ~  factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
summary(model)
par(mfrow = c(2, 2))
plot(model)
autoplot(model)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(broom)
library(ggfortify)
library(ggplot2)
library(tidyr)  # For pivoting the data
library(gridExtra)
library(dplyr)
autoplot(model)
View(data_m.exact_1)
colnames(data_m.exact_1)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 0.9)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 0.1)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 0.9)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 1)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 0.5)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 0.8)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 0.9)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 2)
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = T
)+
stat_smooth(method = "loess", span = 2)
FALSE
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 2)
library(car)
avPlots(model)
facet.by  = c("creation_year", "MS.NonMS"),
avPlots(model)
facet.by  = c("creation_year", "MS.NonMS"),
model <- lm(velocity_mean_end ~  factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
avPlots(model)
facet.by  = c("creation_year", "MS.NonMS"),
avPlots(model)
crPlots(model)
help(crPlots)
crPlot3d(model)
library(rgl)
install.packages(rgl)
install.packages("rgl")
library(rgl)
model <- lm(velocity_mean_end ~  factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
crPlot3d(model)
summary(m.exact_1)
m.exact_1 <- matchit(as.factor(df$MS.NonMS) ~ as.factor(main_language), data = df, method = "exact")
summary(m.exact_1)
autoplot(model)
help("levene_test")
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(model$residuals ~ factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(model$residuals ~ factor(MS.NonMS))
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(data_m.exact_1, model$residuals ~ factor(MS.NonMS))
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(model, model$residuals ~ factor(MS.NonMS))
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(model$coefficients, model$residuals ~ factor(MS.NonMS))
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(data_m.exact_1, model$residuals ~ factor(MS.NonMS))
ggscatter(
data_m.exact_1, x = "velocity_mean_start", y = "velocity_mean_end",
facet.by  = c("creation_year", "MS.NonMS"),
short.panel.labs = FALSE
)+
stat_smooth(method = "loess", span = 2)
autoplot(model)
# 3. Normality of residuals (Normal Q-Q plot)
# The residuals should follow the diagonal line.
shapiro.test(model$residuals)
# Through Shapiro-Wilk test:
shapiro.test(cases_endvelocity) # For cases
# 4. Homogeneity of regression slopes
anova.test <- anova_test(velocity_mean_end ~ factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
anova.test
library(MatchIt)
library(optmatch)
library(Matching)
library(ggplot2)
library(tidyverse)
library(patchwork)
library(plotly)
# Function to generate Q-Q plots for two groups
qq_plot <- function(data, covariate_name, group_column) {
# Filter data for the specific covariate
subset_data <- data[data$Covariate == covariate_name, ]
# Create a QQ plot comparing groups
ggplot(subset_data, aes(sample = Value, colour = !!sym(group_column))) +
stat_qq() +
stat_qq_line() +
labs(title = paste("Q-Q Plot for", covariate_name),
subtitle = "Comparing Quantiles between Groups",
x = "Treatment cases",
y = "Controls") +
scale_fill_continuous(type = "gradient") +
theme_minimal()
}
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
raw_df <- read.csv('/Users/mrobredo23/OULU/docker_cohort-24/data/final_data_file.csv')
View(raw_df)
df <- df[c(2:3, 5:ncol(df))] # For the modelling purposes
cases_df <- df[ df$MS.NonMS =='MS',]
cases_endvelocity <- abs(cases_df$velocity_mean_end)
controls_df <- df[ df$MS.NonMS =='~MS',]
controls_endvelocity <- abs(controls_df$velocity_mean_end)
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
cases_df <- df[ df$MS.NonMS =='MS',]
cases_endvelocity <- abs(cases_df$velocity_mean_end)
cases_df <- df[df$MS.NonMS =='MS',]
cases_endvelocity <- abs(cases_df$velocity_mean_end)
controls_df <- df[df$MS.NonMS =='~MS',]
cases_endvelocity <- cases_df$velocity_mean_end
controls_endvelocity <- controls_df$velocity_mean_end
controls_endvelocity <- controls_df$velocity_mean_end
df <- df[c(2:3, 5:ncol(df))] # For the modelling purposes
View(df)
cases_df <- raw_df[raw_df$MS.NonMS =='MS',]
cases_endvelocity <- abs(cases_df$velocity_mean_end)
# 1. Linearity assumption (Residuals vs Fitted values)
# The blue line should go approximately horizontal and next to 0. If there is
# Some pronounced pattern then means  there is no perfect linearity
autoplot(model)
model <- lm(velocity_mean_end ~  factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
# 1. Linearity assumption (Residuals vs Fitted values)
# The blue line should go approximately horizontal and next to 0. If there is
# Some pronounced pattern then means  there is no perfect linearity
autoplot(model)
# 1. Linearity assumption (Residuals vs Fitted values)
# The blue line should go approximately horizontal and next to 0. If there is
# Some pronounced pattern then means  there is no perfect linearity
autoplot(model)
# 1. Linearity assumption (Residuals vs Fitted values)
# The blue line should go approximately horizontal and next to 0. If there is
# Some pronounced pattern then means  there is no perfect linearity
autoplot(model)
# 1. Linearity assumption (Residuals vs Fitted values)
# The blue line should go approximately horizontal and next to 0. If there is
# Some pronounced pattern then means  there is no perfect linearity
autoplot(model)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(broom)
library(ggfortify)
library(ggplot2)
library(tidyr)  # For pivoting the data
library(gridExtra)
library(dplyr)
library(car)
library(rgl)
model <- lm(velocity_mean_end ~  factor(MS.NonMS) + velocity_mean_start +
size + n_languages + factor(creation_year) + n_commits + n_issues +
n_contributors, data = data_m.exact_1)
plot(model)
# 1. Linearity assumption (Residuals vs Fitted values)
# The blue line should go approximately horizontal and next to 0. If there is
# Some pronounced pattern then means  there is no perfect linearity
autoplot(model)
help(matchit)
# 2. Homogeneity of Variance (Scale-location)
# The line should go horizontal and not follow any trend, therefore showing homogeneity, if not then that means that the residuals have non-constant variance (heterocedasticity)
# Try to 1) Remove outliers, 2) log or square transform of the dependent variable.
levene_test(data_m.exact_1, model$residuals ~ factor(MS.NonMS))
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
hist(controls_endvelocity, col = 'red', main='CONTROLS')
controls_df <- raw_df[raw_df$MS.NonMS =='~MS',]
controls_endvelocity <- abs(controls_df$velocity_mean_end)
hist(controls_endvelocity, col = 'red', main='CONTROLS')
qqnorm(cases_endvelocity, main='CASES')
qqline(cases_endvelocity)
qqnorm(controls_endvelocity, main='CONTROLS')
qqline(controls_endvelocity)
# Through Shapiro-Wilk test:
shapiro.test(cases_endvelocity) # For cases
shapiro.test(controls_endvelocity) # For controls
