cases_cdf <- ecdf(cases)
controls_cdf <- ecdf(controls)
xlim <- range(cont_subset_df[, header_names[i]])
plot(controls_cdf, main = paste('CDF of', header_names[i]), xlab = header_names[i], ylab = 'Cumulative Probability',
col = rgb(1.000000, 0.549020, 0.000000, 0.5), xlim = xlim, verticals = TRUE, do.points = FALSE)
lines(cases_cdf, col = rgb(0.631373, 0.631373, 0.631373, 0.5), verticals = TRUE, do.points = FALSE)
# Add legend
legend("bottomright", legend = c("MS", "~MS"), col = c(rgb(0.631373, 0.631373, 0.631373, 0.5), rgb(1.000000, 0.549020, 0.000000, 0.5)), lty = 1)
}
par(mfrow=c(1, 1))
par(mfrow=c(1, 1))
plot(density(cases_df))
plot(density(cases))
cases
header_names <- c("MS.NonMS", "velocity_mean_start", "size", "n_languages", "n_commits", "n_issues", "n_contributors")
cont_subset_df <- subset(df, select = header_names)
# Kernel density distribution
par(mfrow=c(2, 3))
# header_names[2:length(header_names)]
for (i in seq(from=2, to=length(header_names))){
cases <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "MS"]
controls <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "~MS"]
cases_cdf <- ecdf(cases)
controls_cdf <- ecdf(controls)
xlim <- range(cont_subset_df[, header_names[i]])
plot(controls_cdf, main = paste('CDF of', header_names[i]), xlab = header_names[i], ylab = 'Cumulative Probability',
col = rgb(1.000000, 0.549020, 0.000000, 0.5), xlim = xlim, verticals = TRUE, do.points = FALSE)
lines(cases_cdf, col = rgb(0.631373, 0.631373, 0.631373, 0.5), verticals = TRUE, do.points = FALSE)
# Add legend
legend("bottomright", legend = c("MS", "~MS"), col = c(rgb(0.631373, 0.631373, 0.631373, 0.5), rgb(1.000000, 0.549020, 0.000000, 0.5)), lty = 1)
}
# Cumulative distribution function
par(mfrow=c(2, 3))
# header_names[2:length(header_names)]
for (i in seq(from=2, to=length(header_names))){
cases <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "MS"]
controls <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "~MS"]
cases_cdf <- ecdf(cases)
controls_cdf <- ecdf(controls)
xlim <- range(cont_subset_df[, header_names[i]])
plot(controls_cdf, main = paste('CDF of', header_names[i]), xlab = header_names[i], ylab = 'Cumulative Probability',
col = rgb(1.000000, 0.549020, 0.000000, 0.5), xlim = xlim, verticals = TRUE, do.points = FALSE, lwd=2)
lines(cases_cdf, col = rgb(0.631373, 0.631373, 0.631373, 0.5), verticals = TRUE, do.points = FALSE, lwd=2)
# Add legend
legend("bottomright", legend = c("~MS", "MS"), col = c(rgb(0.631373, 0.631373, 0.631373, 0.5), rgb(1.000000, 0.549020, 0.000000, 0.5)), lty = 1)
}
par(mfrow=c(1, 1))
ks_results
# Sample input list
input_list <- list(
velocity_mean_start = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  velocity_mean_start by MS.NonMS\nD = 0.10154, p-value = 0.7515\nalternative hypothesis: two-sided\n",
size = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  size by MS.NonMS\nD = 0.13876, p-value = 0.3616\nalternative hypothesis: two-sided\n",
n_languages = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  n_languages by MS.NonMS\nD = 0.065125, p-value = 0.9919\nalternative hypothesis: two-sided\n",
creation_year = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  creation_year by MS.NonMS\nD = 0.098086, p-value = 0.7881\nalternative hypothesis: two-sided\n",
n_commits = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  n_commits by MS.NonMS\nD = 0.14593, p-value = 0.3026\nalternative hypothesis: two-sided\n",
n_issues = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  n_issues by MS.NonMS\nD = 0.13876, p-value = 0.3616\nalternative hypothesis: two-sided\n",
n_contributors = "Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  n_contributors by MS.NonMS\nD = 0.11909, p-value = 0.5568\nalternative hypothesis: two-sided\n"
)
# Function to extract the D value and p-value from the string
extract_values <- function(test_result) {
d_value <- as.numeric(sub(".*D = ([0-9.]+),.*", "\\1", test_result))
p_value <- as.numeric(sub(".*p-value = ([0-9.]+).*", "\\1", test_result))
return(c(d_value, p_value))
}
# Initialize an empty data frame
KS_df <- data.frame(
Variable = character(),
D = numeric(),
p_value = numeric(),
stringsAsFactors = FALSE
)
# Loop through the list and populate the data frame
for (variable_name in names(input_list)) {
values <- extract_values(input_list[[variable_name]])
KS_df <- rbind(KS_df, data.frame(
Variable = variable_name,
D = values[1],
p_value = values[2]
))
}
# Print the resulting data frame
print(KS_df)
smd_results
# Sample input list
input_list <- list(
velocity_mean_start = data.frame(
estimate = -0.2626152,
SE = 0.1721281,
lower.ci = -0.5965534,
upper.ci = 0.07265977,
conf.level = 0.95
),
size = data.frame(
estimate = 0.06595153,
SE = 0.1426121,
lower.ci = -0.2117071,
upper.ci = 0.3434563,
conf.level = 0.95
),
n_languages = data.frame(
estimate = -0.1807333,
SE = 0.1661096,
lower.ci = -0.5032995,
upper.ci = 0.1426151,
conf.level = 0.95
),
creation_year = data.frame(
estimate = -0.1987038,
SE = 0.1561734,
lower.ci = -0.5021545,
upper.ci = 0.1053849,
conf.level = 0.95
),
n_commits = data.frame(
estimate = -0.07545033,
SE = 0.1398714,
lower.ci = -0.3476618,
upper.ci = 0.1969286,
conf.level = 0.95
),
n_issues = data.frame(
estimate = 0.02181209,
SE = 0.1553502,
lower.ci = -0.2803865,
upper.ci = 0.3239411,
conf.level = 0.95
),
n_contributors = data.frame(
estimate = -0.1783161,
SE = 0.1653184,
lower.ci = -0.4993677,
upper.ci = 0.1434891,
conf.level = 0.95
)
)
# Initialize an empty data frame
results_df <- data.frame(
Variable = character(),
Estimate = numeric(),
SE = numeric(),
Lower_CI = numeric(),
Upper_CI = numeric(),
Conf_Level = numeric(),
stringsAsFactors = FALSE
)
# Loop through the list and populate the data frame
for (variable_name in names(input_list)) {
values <- input_list[[variable_name]]
results_df <- rbind(results_df, data.frame(
Variable = variable_name,
Estimate = values$estimate,
SE = values$SE,
Lower_CI = values$lower.ci,
Upper_CI = values$upper.ci,
Conf_Level = values$conf.level
))
}
# Print the resulting data frame
print(results_df)
cases_df <- raw_df[raw_df$MS.NonMS =='MS',]
cases_endvelocity <- abs(cases_df$velocity_mean_end)
controls_df <- raw_df[raw_df$MS.NonMS =='~MS',]
controls_endvelocity <- abs(controls_df$velocity_mean_end)
# Through histograms
hist(cases_endvelocity, col='steelblue', main='CASES')
hist(controls_endvelocity, col = 'red', main='CONTROLS')
# Through histograms
par(mfrow=c(1,2))
hist(cases_endvelocity, col="dodgerblue4", main='CASES')
hist(controls_endvelocity, col = "gray63", main='CONTROLS')
# Through interquantile plots
qqnorm(cases_endvelocity, main='CASES', col='blue')
qqline(cases_endvelocity)
qqnorm(controls_endvelocity, main='CONTROLS', col='blue')
qqline(controls_endvelocity)
par(mfrow=c(1,1))
# Through Shapiro-Wilk test: (Given our small sample sizes... it matches the case scenario)
shapiro.test(cases_endvelocity) # For cases
shapiro.test(controls_endvelocity) # For controls
# Anderson-darling (given that it doesn't require sample size)
ad.test(cases_endvelocity)
ad.test(controls_endvelocity)
# D'Agostino's K-squared test:
normalTest(cases_endvelocity, method = c("da")) # has power only against the alternatives that the distribution is skewed and/or kurtic.
normalTest(controls_endvelocity, method = c("da"))
# 3. Sample means are normally distributed (CHECK - They aren't)
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,1))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_sample)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_sample)), add = T, col="red")
par(mfrow=c(1,1))
# 3. Sample means are normally distributed (CHECK - They aren't)
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,1))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_sample)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(cases_sample_mean, main="Sample mean for Cases end velocity", freq=F)
controls_sample <- sample(controls_endvelocity, size = 30)
hist(controls_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample_mean, sd=sd(controls_sample)), add = T, col="red")
hist(controls_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample, sd=sd(controls_sample)), add = T, col="red")
par(mfrow=c(1,1))
# 3. Sample means are normally distributed (CHECK - They aren't)
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,1))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_sample)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(controls_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = controls_sample, sd=sd(controls_sample)), add = T, col="red")
# 3. Sample means are normally distributed (CHECK - They aren't)
# NOTE: According to the CLT a population whose sample of n=30 is normally distributed, it will be approximately distributed.
par(mfrow=c(2,1))
cases_sample <- sample(cases_endvelocity, size = 30)
hist(cases_sample, main="Sample mean for Cases end velocity", freq=F)
curve(dnorm(x, mean = mean(cases_sample), sd=sd(cases_sample)), add = T, col="red")
controls_sample <- sample(controls_endvelocity, size = 30)
hist(controls_sample, main="Sample mean for Controls end velocity", freq=F)
curve(dnorm(x, mean = mean(controls_sample), sd=sd(controls_sample)), add = T, col="red")
par(mfrow=c(1,1))
# 4. The variances between the groups are equal (CHECK - They aren't)
(sd(cases_endvelocity))^2
(sd(controls_endvelocity))^2
##### NON-PARAMETRIC HYPOTHESIS TESTING
library(gridExtra)
plot1 <- ggboxplot(data = df, x = "MS.NonMS", y = "velocity_mean_end",
color = "MS.NonMS", palette = c("dodgerblue4", "gray63"),
ylab = "Velocity end Mean", xlab = "Groups")
plot2 <- ggboxplot(data = df, x = "MS.NonMS", y = "velocity_mean_start",
color = "MS.NonMS", palette = c("dodgerblue4", "gray63"),
ylab = "Velocity start Mean", xlab = "Groups")
grid.arrange(plot1, plot2, ncol = 2)
## Wilcoxon rank sum test (Non parametric) ~ two sample, hence MANN-WHITNEY
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "two.sided", conf.int = T) # p-value = 0.5987
wilcox.test(cases_endvelocity, controls_endvelocity, alternative = "less", conf.int = T) # p-value = 0.7013
# Observational boxplots of the groups.
data_long <- pivot_longer(df, cols = c(velocity_mean_end, velocity_mean_start, size, n_languages, n_commits:n_contributors), names_to = "Covariate", values_to = "Value")
bp1 <- ggplot(data_long, aes(x = Covariate, y = Value, fill = MS.NonMS)) +
geom_boxplot() + # Draw boxplots
theme(axis.text.x = element_blank(), # Hide x-axis text labels
axis.ticks.x = element_blank() # Optionally hide x-axis ticks as well
) + # Rotate x-axis labels for clarity. # axis.text.x = element_text(angle = 45, hjust = 1)
labs(title = "Comparison of Covariates between Treatment and Control Groups",
x = "Covariate",
y = "Value") +
facet_wrap(~ Covariate, scales = "free") # Create a separate plot for each covariate
bp1 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp1 <- ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp1 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp1 <- ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp1 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n())
barp1 <- ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp1 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp1 <- ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp1 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp1 <- ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
# Same based on creation year
df_creation <- df %>%
group_by(creation_year, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp2 <- ggplot(df_creation, aes(x = creation_year, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Creation Year",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp2 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
main_language
df_language
df
raw_df <- subset(raw_df, select = -c(trimmed_languages))
df <- raw_df[c(2:ncol(raw_df))] # For the modelling purposes
# Paired BARPLOT for the main_language confounder in case we get matching
# Count the occurrences
df_language <- df %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp1 <- ggplot(df_language, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Language",
x = "Programming Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp1 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# Same based on number of languages
df_numlang <- df %>%
group_by(n_languages, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
# N_languages
histo_plot <- ggplot(df_numlang, aes(x = n_languages, y= Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.9) +  # Automatically counts; no 'y' aesthetic needed
labs(title = "Distribution of Categories by # of languages",
x = "# of languages",
y = "Count",
fill = "Category") +
theme_minimal() +
scale_fill_manual(values = c("dodgerblue4", "gray63"))
histo_plot
m.exact_1 <- matchit(as.factor(df$MS.NonMS) ~ as.factor(main_language), data = df, method = "exact")
summary(m.exact_1)
# Sample Sizes:
#.              Control Treated
# All            207.        58
# Matched (ESS)  139.44      57
# Matched        198.        57
# Unmatched        9.         1
# Discarded        0.         0
data_m.exact_1 <- as.data.frame(match.data(m.exact_1))
data_long_exact_1 <- pivot_longer(data_m.exact_1, cols = c(velocity_mean_start, size, n_commits:n_contributors), names_to = "Covariate", values_to = "Value")
# Q-Q plot
plots <- lapply(unique(data_long_exact_1$Covariate), function(cov) {
qq_plot(data_long_exact_1, cov, "MS.NonMS")
})
wrap_plots(plots, ncol = 2) # Q-Q plots for matching through different variables (We consider all variables, do not pivot only with one)
# Boxplots
bp_exact <- ggplot(data_long_exact_1, aes(x = Covariate, y = Value, fill = MS.NonMS)) +
geom_boxplot() + # Draw boxplots
theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate x-axis labels for clarity
labs(title = "Comparison of Covariates between Treatment and Control Groups (Exact Matching)",
x = "Covariate",
y = "Value") +
facet_wrap(~ Covariate, scales = "free") # Create a separate plot for each covariate
bp_exact + scale_fill_manual(values = c("dodgerblue4", "gray63"))
# 1. Get row names of the matched data
matched_rows <- rownames(data_m.exact_1)
# 2. Get row names of the original data
original_rows <- rownames(df)
# 3. Find rows that were discarded (those in original not in matched)
discarded_rows <- setdiff(original_rows, matched_rows)
# 4. Extract discarded rows from the original dataframe
discarded_data <- raw_df[discarded_rows, ]
non_discarded_data <- raw_df[matched_rows, ]
# Check the first few rows of the discarded data
head(discarded_data)
# Same based on creation year
df_creation <- non_discarded_data %>%
group_by(creation_year, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp3 <- ggplot(df_creation, aes(x = creation_year, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Creation Year",
x = "Creation year",
y = "Count",
fill = "Category") +
theme_minimal()
barp3 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
df_mainlang <- non_discarded_data %>%
group_by(main_language, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
barp4 <- ggplot(df_mainlang, aes(x = main_language, y = Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
labs(title = "Distribution of Categories by Main Language",
x = "Main Language",
y = "Count",
fill = "Category") +
theme_minimal()
barp4 + scale_fill_manual(values = c("dodgerblue4", "gray63"))
df_numlang <- non_discarded_data %>%
group_by(n_languages, MS.NonMS) %>%
summarise(Count = n(), .groups = 'drop')
# N_languages
histo_plot <- ggplot(df_numlang, aes(x = n_languages, y= Count, fill = as.factor(MS.NonMS))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.9) +  # Automatically counts; no 'y' aesthetic needed
labs(title = "Distribution of Categories by # of languages",
x = "# of languages",
y = "Count",
fill = "Category") +
theme_minimal() +
scale_fill_manual(values = c("dodgerblue4", "gray63"))
histo_plot
confounders <- c("velocity_mean_start", "size", "n_languages", "creation_year", "n_commits", "n_issues", "n_contributors")
smd_results <- list()
ks_results <- list()
for (i in seq_along(confounders)){
formula <- as.formula(paste(confounders[i], "~ MS.NonMS"))
result <- smd_calc(formula = formula, data=non_discarded_data, paired = F,
smd_ci = c("nct"), bias_correction = F)
test_result <- ks.test(formula=formula, data=non_discarded_data)
smd_results[[confounders[i]]] <- result
ks_results[[confounders[i]]] <- test_result
}
header_names <- c("MS.NonMS", "velocity_mean_start", "size", "n_languages", "n_commits", "n_issues", "n_contributors")
cont_subset_df <- subset(df, select = header_names)
# Kernel density distribution
par(mfrow=c(2, 3))
# header_names[2:length(header_names)]
for (i in seq(from=2, to=length(header_names))){
cases <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "MS"]
controls <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "~MS"]
cases_density <- density(cases)
controls_density <- density(controls)
max_density <- max(c(cases_density$y, controls_density$y))
hist(controls, freq = F, main = paste('Histogram of', header_names[i]),
ylab="Kernel density", col = rgb(0.062745, 0.305882, 0.545098, 0.5),
xlab = header_names[i], probability = T,  xlim=range(cont_subset_df[i]),
ylim = c(0, max_density * 1.1))
lines(density(controls), col = "dodgerblue4", lwd=2)
hist(cases, freq = F, col = rgb(1.000000, 0.549020, 0.000000, 0.5), probability = T, add=T)
lines(density(cases), col = "darkorange", lwd=2)
legend("topright", legend = c("~MS", "MS"), fill = c(rgb(0.062745, 0.305882, 0.545098, 0.5), rgb(1.000000, 0.549020, 0.000000, 0.5)))
}
par(mfrow=c(1, 1))
?ecdf
# Cumulative distribution function
par(mfrow=c(2, 3))
# header_names[2:length(header_names)]
for (i in seq(from=2, to=length(header_names))){
cases <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "MS"]
controls <- cont_subset_df[,header_names[i]][cont_subset_df[,header_names[1]] == "~MS"]
cases_cdf <- ecdf(cases)
controls_cdf <- ecdf(controls)
xlim <- range(cont_subset_df[, header_names[i]])
plot(controls_cdf, main = paste('CDF of', header_names[i]), xlab = header_names[i], ylab = 'Cumulative Probability',
col = rgb(1.000000, 0.549020, 0.000000, 0.5), xlim = xlim, verticals = TRUE, do.points = FALSE, lwd=2)
lines(cases_cdf, col = rgb(0.062745, 0.305882, 0.545098, 0.5), verticals = TRUE, do.points = FALSE, lwd=2)
# Add legend
legend("bottomright", legend = c("~MS", "MS"), col = c(rgb(0.062745, 0.305882, 0.545098, 0.5), rgb(1.000000, 0.549020, 0.000000, 0.5)), lty = 1)
}
par(mfrow=c(1, 1))
non_discarded_data
head(non_discarded_data)
matched_data_df <- non_discarded_data
# The df containing the data from the matching process is "matched_data_df"
cases_matched_df <- matched_data_df[matched_data_df$MS.NonMS =='MS',]
cases_matched_endvelocity <- abs(cases_df$velocity_mean_end)
controls_matched_df <- matched_data_df[matched_data_df$MS.NonMS =='~MS',]
controls_matched_endvelocity <- abs(controls_df$velocity_mean_end)
## Wilcoxon rank sum test (Non parametric) ~ two sample, hence MANN-WHITNEY
wilcox.test(cases_matched_endvelocity, controls_matched_endvelocity, alternative = "two.sided", conf.int = T) # p-value = 0.5987
wilcox.test(cases_matched_endvelocity, controls_matched_endvelocity, alternative = "less", conf.int = T) # p-value = 0.7013
length(cases_matched_endvelocity)
length(cases_endvelocity)
length(controls_matched_endvelocity)
length(controls_endvelocity)
non_discarded_data
summary(non_discarded_data)
raw_df <- read.csv('/Users/mrobredo23/OULU/docker_cohort-24/data/final_data_file.csv')
raw_df <- subset(raw_df, select = -c(trimmed_languages))
df <- raw_df[c(2:ncol(raw_df))] # For the modelling purposes
cases_endvelocity <- abs(cases_df$velocity_mean_end)
controls_endvelocity <- abs(controls_df$velocity_mean_end)
length(cases_endvelocity)
length(controls_endvelocity)
length(cases_matched_endvelocity)
length(controls_matched_endvelocity)
help(dhyper)
help("fisher.test")
